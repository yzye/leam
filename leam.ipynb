{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_batch(batch,max_len=80):\n",
    "    x,y = zip(*batch)\n",
    "    x_p = []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(0) < max_len:\n",
    "            x_p.append(\n",
    "            torch.cat([x[i],Variable(torch.tensor([0]*(max_len - x[i].size(0))))]))\n",
    "        else:\n",
    "            x_p.append(x[i][:max_len])\n",
    "    return torch.cat(x_p), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 1e-2\n",
    "num_epoches = 50\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"./yahoo/yahoo.p\"):\n",
    "    with (open(path, \"rb\")) as openfile:\n",
    "        while True:\n",
    "            try:\n",
    "                x = cPickle.load(openfile)\n",
    "            except EOFError:\n",
    "                break\n",
    "        \n",
    "        X, y, test_X, test_lab = x[0], x[3], x[2], x[5]\n",
    "        wordtoix, ixtoword = x[6], x[7]\n",
    "        class_name = ['Good','Bad']\n",
    "        \n",
    "    return X, y, test_X, test_lab, wordtoix, ixtoword, class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, test_X, test_lab, wordtoix, ixtoword, class_name=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[(Variable(torch.tensor((X[i]))),(np.argmax(y[i]))) for i in range(len(y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, class_num, embedding_dim, hidden_dim=100, ngram=55, dropout=0.5):\n",
    "        super(Classifier,self).__init__()\n",
    "        \n",
    "        self.class_num = class_num\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_class = nn.Embedding(class_num, embedding_dim)\n",
    "        \n",
    "        self.conv = torch.nn.Conv1d(class_num, class_num, 2*ngram+1,padding=55)\n",
    "        \n",
    "        self.layer = nn.Linear(embedding_dim, class_num)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        emb = self.embedding(inputs) # (B, L, e)\n",
    "        #print(\"emb1:\",emb)\n",
    "        \n",
    "        embn = torch.norm(emb, p=2, dim=2).detach()        \n",
    "        emb_norm = emb.div(embn.unsqueeze(2))\n",
    "        #print(\"emb2:\",emb)\n",
    "        #print(\"embsize:\",emb.size()) \n",
    "        \n",
    "        emb_c = self.embedding_class(torch.tensor([[i for i in range(self.class_num)] for j in range(inputs.size(0))]))\n",
    "        #print(\"emb_csize:\",emb_c.size()) # (B, C, e)\n",
    "        #print(emb_c)\n",
    "        emb_cn = torch.norm(emb_c, p=2, dim=2).detach()\n",
    "        emb_c_norm = emb_c.div(emb_cn.unsqueeze(2))\n",
    "        \n",
    "        emb_norm_t = emb_norm.permute(0, 2, 1) # (B, e, L)\n",
    "        #print(\"embtsize:\",embt.size())\n",
    "        \n",
    "        g = torch.bmm(emb_c_norm,emb_norm_t) #(B, C, L)\n",
    "        #print(\"gsize:\",g.size())\n",
    "        \n",
    "        g = F.relu(self.conv(g))\n",
    "        \n",
    "        beta = torch.max(g,1)[0].unsqueeze(2) #(B, L)\n",
    "        \n",
    "        #print(\"betasize:\",beta.size())\n",
    "        beta = F.softmax(beta,1) #(B, L)\n",
    "        \n",
    "        z = torch.mul(beta,emb) #(B, L, e)\n",
    "        #print(\"z1size:\",z.size())\n",
    "        \n",
    "        z = z.sum(1) #(B, e)\n",
    "        #print(\"z2size:\",z.size())\n",
    "        \n",
    "        out = self.layer(z) #(B, C)\n",
    "        #print(\"outsize:\",out.size())\n",
    "        \n",
    "        logits = F.log_softmax(out,1) #(B, C)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(vocab_size=len(wordtoix), class_num=5, embedding_dim=300, hidden_dim=100, ngram=55, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    print(\"--- epoch:\",epoch,\"---\")\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "    for i, data in enumerate(getBatch(batch_size, train_data), 1):\n",
    "\n",
    "        inputs,targets = pad_to_batch(data)\n",
    "        model.zero_grad()\n",
    "\n",
    "        preds = model(inputs.view(batch_size,-1)) #(B, C)\n",
    "        #targets: (B,)\n",
    "        \n",
    "        if len(targets)!= batch_size:\n",
    "            break\n",
    "\n",
    "        loss = loss_function(preds, torch.LongTensor(targets))\n",
    "\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        max_index = preds.max(dim = 1)[1]\n",
    "        correct = (max_index == torch.LongTensor(targets)).sum()\n",
    "        acc = float(correct)/batch_size\n",
    "        accuracy.append(acc)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "            losses = []\n",
    "    \n",
    "    loss_epoch = np.mean(losses)\n",
    "    print(\"loss_epoch:\",loss_epoch)\n",
    "    acc_epoch = np.mean(accuracy)\n",
    "    print(\"acc_epoch:\",acc_epoch)\n",
    "    \n",
    "torch.save(model.state_dict(),\"checkpoints/trained_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[(Variable(torch.tensor((test_X[i]))),(np.argmax(test_lab[i]))) for i in range(len(test_lab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(batch_size, test_data, model):\n",
    "    acc = []\n",
    "    \n",
    "    for i, data in enumerate(getBatch(batch_size, test_data), 1):\n",
    "        \n",
    "        inputs,targets = pad_to_batch(data)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #print(\"inputs:\",inputs)\n",
    "        preds = model(inputs.view(batch_size,-1))\n",
    "        \n",
    "        max_index = preds.max(dim = 1)[1]\n",
    "        \n",
    "        if len(targets)== batch_size:\n",
    "            correct = (max_index == torch.LongTensor(targets)).sum()\n",
    "            acc.append(float(correct)/batch_size)\n",
    "        \n",
    "    return np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = Classifier(vocab_size=len(wordtoix), class_num=5, embedding_dim=300, hidden_dim=100, ngram=55, dropout=0.5)\n",
    "trained_model.load_state_dict(torch.load(\"checkpoints/trained_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy(batch_size, test_data, trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
